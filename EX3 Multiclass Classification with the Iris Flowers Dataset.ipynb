{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Básico #EX03\n",
    "# Multiclass Classification Of Flower Species\n",
    "\n",
    "# Clasificación Multiclase de Especies Florales\n",
    "\n",
    "En este tutorial de proyecto, descubrirá cómo puede usar Keras para desarrollar y evaluar modelos de redes neuronales para problemas de clasificación de clases múltiples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de datos de clasificación de flores de iris\n",
    "\n",
    "En este tutorial utilizaremos el problema de aprendizaje automático estándar denominado conjunto de datos de flores iris. Este conjunto de datos está bien estudiado y es un buen problema para practicar en redes neuronales porque las 4 variables de entrada son numéricas y tienen la misma escala en centímetros. Cada instancia describe las propiedades de una medición.\n",
    "Las mediciones de flujo y la variable de salida son especies específicas de iris. Los atributos para este conjunto de datos se pueden resumir de la siguiente manera:\n",
    "\n",
    "1. Longitud del sépalo en centímetros.\n",
    "2. Ancho sepal en centímetros.\n",
    "3. Longitud del pétalo en centímetros.\n",
    "4. Ancho del pétalo en centímetros.\n",
    "5. Clase.\n",
    "\n",
    "Este es un problema de clasificación multiclase, lo que significa que hay más de dos clases para predecir, de hecho, hay tres especies de flores. Este es un tipo importante de problema para practicar con redes neuronales porque los tres valores de clase requieren un manejo especializado. A continuación se muestra una muestra de las primeras cinco de las 150 instancias:\n",
    "\n",
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "\n",
    "4.9,3.0,1.4,0.2,Iris-setosa\n",
    "\n",
    "4.7,3.2,1.3,0.2,Iris-setosa\n",
    "\n",
    "4.6,3.1,1.5,0.2,Iris-setosa\n",
    "\n",
    "5.0,3.6,1.4,0.2,Iris-setosa\n",
    "\n",
    "\n",
    "El conjunto de datos de la flor del iris es un problema bien estudiado y podemos esperar una precisión del modelo en el rango del 95% al 97%. Esto proporciona un buen objetivo para obtener durante el desarrollo de nuestros modelos. Puede descargar el conjunto de datos de flores del iris desde el repositorio de Aprendizaje automático de UCI y ubicarlo en su directorio de trabajo actual con el nombre de archivo iris.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Clases y Funciones\n",
    "\n",
    "Podemos comenzar importando todas las clases y funciones que necesitaremos en este tutorial. Esto incluye tanto la funcionalidad que requerimos de Keras, como la carga de datos desde Pandas, así como la preparación de datos y la evaluación del modelo desde scikit-learn.\n",
    "\n",
    "Además, necesitamos inicializar el generador de números aleatorios a un valor constante. Esto es importante para garantizar que los resultados que obtenemos de este modelo se puedan lograr nuevamente de manera precisa. Además, asegura que el proceso estocástico de entrenamiento de un modelo de red neuronal pueda reproducirse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification with the Iris Flowers Dataset\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar el conjunto de datos\n",
    "\n",
    "El conjunto de datos se puede cargar directamente. Debido a que la variable de salida contiene cadenas (strings), es más fácil cargar los datos utilizando pandas. Luego podemos dividir los atributos (columnas) en variables de entrada (X) y variables de salida (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codificar la variable de salida\n",
    "\n",
    "La variable de salida contiene tres valores de cadena diferentes. Al modelar problemas de clasificación multiclase usando redes neuronales, es una buena práctica remodelar el atributo de salida de un vector que contiene valores para cada valor de clase para que sea una matriz con un valor booleano para cada valor de clase y si una instancia dada tiene esa clase o no. Esto se denomina codificación en caliente (one hot encoding) o creación de variables ficticias a partir de una variable categórica. Por ejemplo, en este problema, los tres valores de clase son Iris-setosa, Iris-versicolor e Iris-virginica. Si tuviéramos las tres observaciones:\n",
    "\n",
    "Iris-setosa\n",
    "\n",
    "Iris-versicolor\n",
    "\n",
    "Iris-virginica\n",
    "\n",
    "Podemos convertir esto en una matriz binaria codificada para cada instancia de datos que se vería de la siguiente manera:\n",
    "\n",
    "Iris-setosa, Iris-versicolor, Iris-virginica\n",
    "\n",
    "1,                  0,              0\n",
    "\n",
    "0,                  1,              0\n",
    "\n",
    "0,                  0,              1\n",
    "\n",
    "Podemos hacer esto mediante la primera codificación de las cadenas de forma coherente a enteros utilizando la clase LabelEncoder de scikit-learn. Luego convierta el vector de enteros en una codificación one hot encoding utilizando la función Keras to_categorical().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir el modelo de red neuronal\n",
    "\n",
    "La biblioteca de Keras proporciona clases para permitirle usar modelos de redes neuronales desarrollados con Keras en scikit-learn como vimos en el ejemplo (clase) anterior. Hay una clase KerasClassifier en Keras que se puede usar como un Estimador en scikit-learn, el tipo base de modelo en la biblioteca. El KerasClassifier toma el nombre de una función como un argumento. Esta función debe devolver el modelo de red neuronal construido, listo para el entrenamiento.\n",
    "\n",
    "A continuación se muestra una función que creará una red neuronal base para el problema de clasificación de iris. Crea una red simple completamente conectada con una capa oculta que contiene 4 neuronas, el mismo número de entradas (podría ser cualquier número de neuronas). La capa oculta utiliza una función de activación de rectificador lo cual es una buena práctica. Debido a que usamos una codificación one hot para nuestro conjunto de datos de iris, la capa de salida debe crear 3 valores de salida, uno para cada clase. El valor de salida con el valor más grande se tomará como la clase predicha por el modelo. La topología de red de esta simple red neuronal de una capa se puede resumir como:\n",
    "\n",
    "4 inputs -> [4 hidden nodes] -> 3 outputs\n",
    "\n",
    "Tenga en cuenta que usamos una función de activación sigmoide en la capa de salida. Esto es para asegurar que los valores de salida estén en el rango de 0 y 1 y se puedan usar como probabilidades pronosticadas. Finalmente, la red utiliza el eficiente algoritmo de optimización de descenso por gradiente de ADAM con una función de pérdida logarítmica, que se denomina crossentropy categórica en Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, init='normal', activation='relu'))\n",
    "    model.add(Dense(3, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimador\n",
    "\n",
    "Ahora podemos crear nuestro KerasClassifier para su uso en scikit-learn. También podemos pasar argumentos en la construcción de la clase KerasClassifier que se pasará a la función fit() internamente utilizada para entrenar la red neuronal. Aquí, pasamos el número de épocas como 200 y tamaño de lote como 5 para usar al entrenar el modelo. La depuración también se desactiva cuando se entrena estableciendo verbose en 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar el modelo con la validación cruzada k-Fold\n",
    "\n",
    "Ahora podemos evaluar el modelo de red neuronal en nuestros datos de entrenamiento. La biblioteca scikit-learn tiene una excelente capacidad para evaluar modelos utilizando un conjunto de técnicas. El estándar de oro para evaluar modelos de aprendizaje automático es la validación cruzada k-fold. Primero podemos definir el procedimiento de evaluación del modelo. Aquí, establecemos que el número de carpetas sea 10 (un valor predeterminado excelente) y que mezclen los datos antes de particionarlos.\n",
    "\n",
    "Luego podemos evaluar nuestro modelo (estimador) en nuestro conjunto de datos (X y dummy_y) utilizando el procedimiento de validación cruzada de 10 veces (kfold). La evaluación del modelo solo toma algunos minutos y devuelve un objeto que describe la evaluación de los 10 modelos construidos para cada una de las divisiones del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código completo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification with the Iris Flowers Dataset\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "#print(X)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, init='normal', activation='relu'))\n",
    "    model.add(Dense(3, init='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar el modelo directamente\n",
    "\n",
    "Ejemplo de como generar una red neuronal que clasifique multiples clases directamente, sin validación cruzada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, input_dim=4, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "150/150 [==============================] - 2s 16ms/step - loss: 1.0995 - acc: 0.2067\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0962 - acc: 0.3600\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0916 - acc: 0.3333\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0845 - acc: 0.3333\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0732 - acc: 0.3533\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0570 - acc: 0.6333\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0366 - acc: 0.6667\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0129 - acc: 0.6667\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.9857 - acc: 0.6667\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.9559 - acc: 0.6667\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.9239 - acc: 0.6667\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8911 - acc: 0.6667\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8568 - acc: 0.6667\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8230 - acc: 0.6667\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7895 - acc: 0.6667\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.7565 - acc: 0.6667\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7251 - acc: 0.6733\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.6951 - acc: 0.6933\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.6676 - acc: 0.7000\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.6410 - acc: 0.7067\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.6170 - acc: 0.7933\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5943 - acc: 0.8467\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5744 - acc: 0.9533\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5549 - acc: 0.9533\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5374 - acc: 0.9667\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5219 - acc: 0.9667\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5060 - acc: 0.9733\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4921 - acc: 0.9733\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.4796 - acc: 0.9533\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.4666 - acc: 0.9800\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4547 - acc: 0.9733\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4445 - acc: 0.9733\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4327 - acc: 0.9667\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4241 - acc: 0.9667\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4128 - acc: 0.9800\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4028 - acc: 0.9800\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3956 - acc: 0.9800\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3857 - acc: 0.9667\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.3787 - acc: 0.9667\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3701 - acc: 0.9667\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3632 - acc: 0.9733\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3557 - acc: 0.9733\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.3509 - acc: 0.9733\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3416 - acc: 0.9800\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3375 - acc: 0.9667\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3293 - acc: 0.9733\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3232 - acc: 0.9733\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3177 - acc: 0.9733\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3132 - acc: 0.9667\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3083 - acc: 0.9733\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3021 - acc: 0.9800\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2987 - acc: 0.9733\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.2935 - acc: 0.9733\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2881 - acc: 0.9667\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2853 - acc: 0.9800\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.2801 - acc: 0.9733\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.2777 - acc: 0.9667\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.2717 - acc: 0.9667\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.2671 - acc: 0.9733\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.2640 - acc: 0.9733\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.2601 - acc: 0.9733\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2619 - acc: 0.9733\n",
      "Epoch 63/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.2540 - acc: 0.9667\n",
      "Epoch 64/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2507 - acc: 0.9733\n",
      "Epoch 65/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2464 - acc: 0.9733\n",
      "Epoch 66/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.2441 - acc: 0.9733\n",
      "Epoch 67/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.2404 - acc: 0.9733\n",
      "Epoch 68/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2395 - acc: 0.9733\n",
      "Epoch 69/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2348 - acc: 0.9733\n",
      "Epoch 70/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2332 - acc: 0.9800\n",
      "Epoch 71/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2295 - acc: 0.9733\n",
      "Epoch 72/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2269 - acc: 0.9667\n",
      "Epoch 73/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.2236 - acc: 0.9733\n",
      "Epoch 74/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.2211 - acc: 0.9733\n",
      "Epoch 75/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.2188 - acc: 0.9733\n",
      "Epoch 76/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2164 - acc: 0.9733\n",
      "Epoch 77/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2154 - acc: 0.9667\n",
      "Epoch 78/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.2130 - acc: 0.9733\n",
      "Epoch 79/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2106 - acc: 0.9733\n",
      "Epoch 80/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2122 - acc: 0.9733\n",
      "Epoch 81/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2054 - acc: 0.9667\n",
      "Epoch 82/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2035 - acc: 0.9667\n",
      "Epoch 83/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2010 - acc: 0.9733\n",
      "Epoch 84/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1998 - acc: 0.9733\n",
      "Epoch 85/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1991 - acc: 0.9867\n",
      "Epoch 86/200\n",
      "150/150 [==============================] - 0s 931us/step - loss: 0.1958 - acc: 0.9733\n",
      "Epoch 87/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1949 - acc: 0.9733\n",
      "Epoch 88/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1921 - acc: 0.9733\n",
      "Epoch 89/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1902 - acc: 0.9733\n",
      "Epoch 90/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1893 - acc: 0.9733\n",
      "Epoch 91/200\n",
      "150/150 [==============================] - 0s 924us/step - loss: 0.1866 - acc: 0.9733\n",
      "Epoch 92/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1845 - acc: 0.9733\n",
      "Epoch 93/200\n",
      "150/150 [==============================] - 0s 990us/step - loss: 0.1839 - acc: 0.9733\n",
      "Epoch 94/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1823 - acc: 0.9667\n",
      "Epoch 95/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.1836 - acc: 0.9667\n",
      "Epoch 96/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1795 - acc: 0.9733\n",
      "Epoch 97/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1783 - acc: 0.9733\n",
      "Epoch 98/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1758 - acc: 0.9733\n",
      "Epoch 99/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.1770 - acc: 0.9733\n",
      "Epoch 100/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1741 - acc: 0.9667\n",
      "Epoch 101/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1758 - acc: 0.9667\n",
      "Epoch 102/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.1704 - acc: 0.9733\n",
      "Epoch 103/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1688 - acc: 0.9733\n",
      "Epoch 104/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1691 - acc: 0.9733\n",
      "Epoch 105/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1671 - acc: 0.9733\n",
      "Epoch 106/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1652 - acc: 0.9667\n",
      "Epoch 107/200\n",
      "150/150 [==============================] - 0s 931us/step - loss: 0.1642 - acc: 0.9733\n",
      "Epoch 108/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.1630 - acc: 0.9733\n",
      "Epoch 109/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.1621 - acc: 0.9733\n",
      "Epoch 110/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1603 - acc: 0.9667\n",
      "Epoch 111/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.1603 - acc: 0.9667\n",
      "Epoch 112/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1594 - acc: 0.9667\n",
      "Epoch 113/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.1581 - acc: 0.9733\n",
      "Epoch 114/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1559 - acc: 0.9733\n",
      "Epoch 115/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1573 - acc: 0.9667\n",
      "Epoch 116/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1547 - acc: 0.9667\n",
      "Epoch 117/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1547 - acc: 0.9667\n",
      "Epoch 118/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.1519 - acc: 0.9733\n",
      "Epoch 119/200\n",
      "150/150 [==============================] - 0s 957us/step - loss: 0.1516 - acc: 0.9667\n",
      "Epoch 120/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1520 - acc: 0.9733\n",
      "Epoch 121/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1497 - acc: 0.9733\n",
      "Epoch 122/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1498 - acc: 0.9733\n",
      "Epoch 123/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1498 - acc: 0.9800\n",
      "Epoch 124/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1464 - acc: 0.9667\n",
      "Epoch 125/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1461 - acc: 0.9733\n",
      "Epoch 126/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.1446 - acc: 0.9733\n",
      "Epoch 127/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1442 - acc: 0.9733\n",
      "Epoch 128/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1427 - acc: 0.9733\n",
      "Epoch 129/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1447 - acc: 0.9733\n",
      "Epoch 130/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1426 - acc: 0.9733\n",
      "Epoch 131/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1416 - acc: 0.9733\n",
      "Epoch 132/200\n",
      "150/150 [==============================] - 0s 898us/step - loss: 0.1396 - acc: 0.9733\n",
      "Epoch 133/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.1402 - acc: 0.9733\n",
      "Epoch 134/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1397 - acc: 0.9800\n",
      "Epoch 135/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1388 - acc: 0.9733\n",
      "Epoch 136/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1373 - acc: 0.9667\n",
      "Epoch 137/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1364 - acc: 0.9667\n",
      "Epoch 138/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1347 - acc: 0.9733\n",
      "Epoch 139/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1358 - acc: 0.9667\n",
      "Epoch 140/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.1391 - acc: 0.9667\n",
      "Epoch 141/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1326 - acc: 0.9667\n",
      "Epoch 142/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1327 - acc: 0.9667\n",
      "Epoch 143/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1309 - acc: 0.9667\n",
      "Epoch 144/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1307 - acc: 0.9667\n",
      "Epoch 145/200\n",
      "150/150 [==============================] - 0s 931us/step - loss: 0.1316 - acc: 0.9733\n",
      "Epoch 146/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1315 - acc: 0.9667\n",
      "Epoch 147/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1279 - acc: 0.9733\n",
      "Epoch 148/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.1315 - acc: 0.9733\n",
      "Epoch 149/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1293 - acc: 0.9733\n",
      "Epoch 150/200\n",
      "150/150 [==============================] - 0s 918us/step - loss: 0.1277 - acc: 0.9667\n",
      "Epoch 151/200\n",
      "150/150 [==============================] - 0s 958us/step - loss: 0.1276 - acc: 0.9733\n",
      "Epoch 152/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.1271 - acc: 0.9667\n",
      "Epoch 153/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1255 - acc: 0.9667\n",
      "Epoch 154/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.1242 - acc: 0.9800\n",
      "Epoch 155/200\n",
      "150/150 [==============================] - 0s 970us/step - loss: 0.1242 - acc: 0.9667\n",
      "Epoch 156/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1247 - acc: 0.9667\n",
      "Epoch 157/200\n",
      "150/150 [==============================] - 0s 911us/step - loss: 0.1233 - acc: 0.9667\n",
      "Epoch 158/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1229 - acc: 0.9733\n",
      "Epoch 159/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1221 - acc: 0.9667\n",
      "Epoch 160/200\n",
      "150/150 [==============================] - 0s 911us/step - loss: 0.1225 - acc: 0.9667\n",
      "Epoch 161/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.1202 - acc: 0.9667\n",
      "Epoch 162/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.1200 - acc: 0.9667\n",
      "Epoch 163/200\n",
      "150/150 [==============================] - 0s 957us/step - loss: 0.1235 - acc: 0.9800\n",
      "Epoch 164/200\n",
      "150/150 [==============================] - 0s 898us/step - loss: 0.1202 - acc: 0.9733\n",
      "Epoch 165/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1187 - acc: 0.9667\n",
      "Epoch 166/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1200 - acc: 0.9667\n",
      "Epoch 167/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.1181 - acc: 0.9667\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 884us/step - loss: 0.1163 - acc: 0.9667\n",
      "Epoch 169/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1173 - acc: 0.9733\n",
      "Epoch 170/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1168 - acc: 0.9667\n",
      "Epoch 171/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1158 - acc: 0.9667\n",
      "Epoch 172/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.1159 - acc: 0.9667\n",
      "Epoch 173/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1145 - acc: 0.9733\n",
      "Epoch 174/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1141 - acc: 0.9667\n",
      "Epoch 175/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1146 - acc: 0.9667\n",
      "Epoch 176/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1178 - acc: 0.9800\n",
      "Epoch 177/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1136 - acc: 0.9733\n",
      "Epoch 178/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1135 - acc: 0.9667\n",
      "Epoch 179/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1127 - acc: 0.9667\n",
      "Epoch 180/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.1117 - acc: 0.9667\n",
      "Epoch 181/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1118 - acc: 0.9667\n",
      "Epoch 182/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1117 - acc: 0.9800\n",
      "Epoch 183/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1110 - acc: 0.9733\n",
      "Epoch 184/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1103 - acc: 0.9667\n",
      "Epoch 185/200\n",
      "150/150 [==============================] - 0s 957us/step - loss: 0.1120 - acc: 0.9667\n",
      "Epoch 186/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.1100 - acc: 0.9667\n",
      "Epoch 187/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1085 - acc: 0.9667\n",
      "Epoch 188/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1087 - acc: 0.9667\n",
      "Epoch 189/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1088 - acc: 0.9667\n",
      "Epoch 190/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1082 - acc: 0.9733\n",
      "Epoch 191/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1083 - acc: 0.9733\n",
      "Epoch 192/200\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.1077 - acc: 0.9800\n",
      "Epoch 193/200\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.1074 - acc: 0.9733\n",
      "Epoch 194/200\n",
      "150/150 [==============================] - 0s 937us/step - loss: 0.1076 - acc: 0.9667\n",
      "Epoch 195/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.1061 - acc: 0.9733\n",
      "Epoch 196/200\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.1055 - acc: 0.9667\n",
      "Epoch 197/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1057 - acc: 0.9667\n",
      "Epoch 198/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.1076 - acc: 0.9667\n",
      "Epoch 199/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1051 - acc: 0.9733\n",
      "Epoch 200/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1066 - acc: 0.9733\n",
      "150/150 [==============================] - 0s 246us/step\n",
      "acc: 96.67%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# define baseline model\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=4, init='normal', activation='relu'))\n",
    "model.add(Dense(3, init='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, dummy_y, nb_epoch=200, batch_size=5)  \n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X,  dummy_y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisión de predicción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.19515116e-12, 4.67439677e-04, 1.14623785e-01]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.predict(np.reshape(X[100], [1, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
